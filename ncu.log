==PROF== Connected to process 395991 (/opt/conda/bin/python3.8)
==PROF== Profiling "distribution_elementwise_grid..." - 0: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 1: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 2: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 3: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 4: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 5: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 6: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 7: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 8: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 9: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 10: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 11: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 12: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 13: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 14: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 15: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 16: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 17: 0%....50%....100% - 10 passes
==PROF== Profiling "kernel1" - 18: 0%....50%....100% - 10 passes
==PROF== Profiling "kernel1" - 19: 0%....50%....100% - 10 passes
==PROF== Profiling "kernel1" - 20: 0%....50%....100% - 10 passes
==PROF== Profiling "kernel1" - 21: 0%....50%....100% - 10 passes
==PROF== Profiling "kernel1" - 22: 0%....50%....100% - 10 passes
==PROF== Profiling "kernel1" - 23: 0%....50%....100% - 10 passes
==PROF== Profiling "kernel1" - 24: 0%....50%....100% - 10 passes
==PROF== Profiling "kernel1" - 25: 0%....50%....100% - 10 passes
==PROF== Profiling "kernel1" - 26: 0%....50%....100% - 10 passes
==PROF== Profiling "kernel1" - 27: 0%....50%....100% - 10 passes
==PROF== Profiling "kernel1" - 28: 0%....50%....100% - 10 passes
==PROF== Profiling "kernel1" - 29: 0%....50%....100% - 10 passes
==PROF== Profiling "kernel1" - 30: 0%....50%....100% - 10 passes
shape= 2048 x 2048 bw_eager= 0.018804305111252216 bw_fuser= 0.018693567365063755 GB/s, ratio= 0.994111042895055
==PROF== Disconnected from process 395991
[395991] python3.8@127.0.0.1
  void at::native::<unnamed>::distribution_elementwise_grid_stride_kernel<float, (int)4, void at::native::templates::cuda::normal_and_transform<c10::Half, float, (unsigned long)4, at::CUDAGeneratorImpl *, void at::native::templates::cuda::normal_kernel<at::CUDAGeneratorImpl *>(const at::TensorBase &, double, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 6)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, T5)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void at::native::<unnamed>::distribution_nullary_kernel<c10::Half, float, (int)4, at::CUDAGeneratorImpl *, void at::native::templates::cuda::normal_and_transform<c10::Half, float, (unsigned long)4, at::CUDAGeneratorImpl *, void at::native::templates::cuda::normal_kernel<at::CUDAGeneratorImpl *>(const at::TensorBase &, double, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 6)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, T5)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void at::native::templates::cuda::normal_kernel<at::CUDAGeneratorImpl *>(const at::TensorBase &, double, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 6)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(int, at::PhiloxCudaState, T3, T4), 2022-Dec-04 20:05:20, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.50
    SM Frequency                                                             cycle/nsecond                           1.05
    Elapsed Cycles                                                                   cycle                          30988
    Memory [%]                                                                           %                          11.62
    DRAM Throughput                                                                      %                           0.00
    Duration                                                                       usecond                          29.38
    L1/TEX Cache Throughput                                                              %                           8.70
    L2 Cache Throughput                                                                  %                          18.87
    SM Active Cycles                                                                 cycle                       27953.58
    Compute (SM) [%]                                                                     %                          64.22
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis section to see what the   
          compute pipelines are spending their time doing. Also, consider whether any computation is redundant and      
          could be reduced or moved to look-up tables.                                                                  

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        256
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         864
    Registers Per Thread                                                   register/thread                             38
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         221184
    Waves Per SM                                                                                                     1.33
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 215 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              6
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              8
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          58.21
    Achieved Active Warps Per SM                                                      warp                          37.26
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers The difference     
          between calculated theoretical (75.0%) and measured achieved occupancy (58.2%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void at::native::<unnamed>::vectorized_layer_norm_kernel<c10::Half, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *), 2022-Dec-04 20:05:21, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.53
    SM Frequency                                                             cycle/nsecond                           1.06
    Elapsed Cycles                                                                   cycle                          25319
    Memory [%]                                                                           %                          37.79
    DRAM Throughput                                                                      %                          18.11
    Duration                                                                       usecond                          23.68
    L1/TEX Cache Throughput                                                              %                          44.76
    L2 Cache Throughput                                                                  %                          35.47
    SM Active Cycles                                                                 cycle                       21196.01
    Compute (SM) [%]                                                                     %                          55.40
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        2048
    Registers Per Thread                                                   register/thread                             33
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                             24
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         262144
    Waves Per SM                                                                                                     1.58
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 751 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             12
    Block Limit Shared Mem                                                           block                             28
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          57.99
    Achieved Active Warps Per SM                                                      warp                          37.11
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers The difference     
          between calculated theoretical (75.0%) and measured achieved occupancy (58.0%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void at::native::<unnamed>::vectorized_layer_norm_kernel<c10::Half, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *), 2022-Dec-04 20:05:22, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.53
    SM Frequency                                                             cycle/nsecond                           1.06
    Elapsed Cycles                                                                   cycle                          25582
    Memory [%]                                                                           %                          37.50
    DRAM Throughput                                                                      %                          17.92
    Duration                                                                       usecond                          23.94
    L1/TEX Cache Throughput                                                              %                          45.06
    L2 Cache Throughput                                                                  %                          34.75
    SM Active Cycles                                                                 cycle                       21128.99
    Compute (SM) [%]                                                                     %                          54.79
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        2048
    Registers Per Thread                                                   register/thread                             33
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                             24
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         262144
    Waves Per SM                                                                                                     1.58
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 751 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             12
    Block Limit Shared Mem                                                           block                             28
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          58.22
    Achieved Active Warps Per SM                                                      warp                          37.26
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers The difference     
          between calculated theoretical (75.0%) and measured achieved occupancy (58.2%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void at::native::<unnamed>::vectorized_layer_norm_kernel<c10::Half, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *), 2022-Dec-04 20:05:23, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.46
    SM Frequency                                                             cycle/nsecond                           1.01
    Elapsed Cycles                                                                   cycle                          24887
    Memory [%]                                                                           %                          38.55
    DRAM Throughput                                                                      %                          18.43
    Duration                                                                       usecond                          24.38
    L1/TEX Cache Throughput                                                              %                          44.80
    L2 Cache Throughput                                                                  %                          35.55
    SM Active Cycles                                                                 cycle                       21225.68
    Compute (SM) [%]                                                                     %                          56.38
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        2048
    Registers Per Thread                                                   register/thread                             33
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                             24
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         262144
    Waves Per SM                                                                                                     1.58
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 751 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             12
    Block Limit Shared Mem                                                           block                             28
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          58.02
    Achieved Active Warps Per SM                                                      warp                          37.13
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers The difference     
          between calculated theoretical (75.0%) and measured achieved occupancy (58.0%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void at::native::<unnamed>::vectorized_layer_norm_kernel<c10::Half, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *), 2022-Dec-04 20:05:24, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.50
    SM Frequency                                                             cycle/nsecond                           1.04
    Elapsed Cycles                                                                   cycle                          24812
    Memory [%]                                                                           %                          38.52
    DRAM Throughput                                                                      %                          18.47
    Duration                                                                       usecond                          23.68
    L1/TEX Cache Throughput                                                              %                          44.70
    L2 Cache Throughput                                                                  %                          36.39
    SM Active Cycles                                                                 cycle                       21217.21
    Compute (SM) [%]                                                                     %                          56.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        2048
    Registers Per Thread                                                   register/thread                             33
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                             24
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         262144
    Waves Per SM                                                                                                     1.58
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 751 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             12
    Block Limit Shared Mem                                                           block                             28
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          57.93
    Achieved Active Warps Per SM                                                      warp                          37.07
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers The difference     
          between calculated theoretical (75.0%) and measured achieved occupancy (57.9%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void at::native::<unnamed>::vectorized_layer_norm_kernel<c10::Half, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *), 2022-Dec-04 20:05:25, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.49
    SM Frequency                                                             cycle/nsecond                           1.03
    Elapsed Cycles                                                                   cycle                          25431
    Memory [%]                                                                           %                          37.66
    DRAM Throughput                                                                      %                          18.04
    Duration                                                                       usecond                          24.42
    L1/TEX Cache Throughput                                                              %                          44.61
    L2 Cache Throughput                                                                  %                          34.61
    SM Active Cycles                                                                 cycle                       21277.86
    Compute (SM) [%]                                                                     %                          55.23
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        2048
    Registers Per Thread                                                   register/thread                             33
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                             24
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         262144
    Waves Per SM                                                                                                     1.58
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 751 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             12
    Block Limit Shared Mem                                                           block                             28
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          58.03
    Achieved Active Warps Per SM                                                      warp                          37.14
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers The difference     
          between calculated theoretical (75.0%) and measured achieved occupancy (58.0%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void at::native::<unnamed>::vectorized_layer_norm_kernel<c10::Half, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *), 2022-Dec-04 20:05:26, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.48
    SM Frequency                                                             cycle/nsecond                           1.03
    Elapsed Cycles                                                                   cycle                          25160
    Memory [%]                                                                           %                          37.97
    DRAM Throughput                                                                      %                          18.24
    Duration                                                                       usecond                          24.29
    L1/TEX Cache Throughput                                                              %                          44.52
    L2 Cache Throughput                                                                  %                          34.05
    SM Active Cycles                                                                 cycle                       21269.87
    Compute (SM) [%]                                                                     %                          55.78
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        2048
    Registers Per Thread                                                   register/thread                             33
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                             24
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         262144
    Waves Per SM                                                                                                     1.58
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 751 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             12
    Block Limit Shared Mem                                                           block                             28
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          57.81
    Achieved Active Warps Per SM                                                      warp                          37.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers The difference     
          between calculated theoretical (75.0%) and measured achieved occupancy (57.8%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void at::native::<unnamed>::vectorized_layer_norm_kernel<c10::Half, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *), 2022-Dec-04 20:05:27, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.51
    SM Frequency                                                             cycle/nsecond                           1.05
    Elapsed Cycles                                                                   cycle                          25430
    Memory [%]                                                                           %                          37.59
    DRAM Throughput                                                                      %                          18.04
    Duration                                                                       usecond                          24.10
    L1/TEX Cache Throughput                                                              %                          44.55
    L2 Cache Throughput                                                                  %                          35.39
    SM Active Cycles                                                                 cycle                       21294.54
    Compute (SM) [%]                                                                     %                          55.12
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        2048
    Registers Per Thread                                                   register/thread                             33
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                             24
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         262144
    Waves Per SM                                                                                                     1.58
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 751 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             12
    Block Limit Shared Mem                                                           block                             28
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          57.93
    Achieved Active Warps Per SM                                                      warp                          37.07
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers The difference     
          between calculated theoretical (75.0%) and measured achieved occupancy (57.9%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void at::native::<unnamed>::vectorized_layer_norm_kernel<c10::Half, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *), 2022-Dec-04 20:05:28, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.49
    SM Frequency                                                             cycle/nsecond                           1.03
    Elapsed Cycles                                                                   cycle                          24997
    Memory [%]                                                                           %                          38.23
    DRAM Throughput                                                                      %                          18.32
    Duration                                                                       usecond                          24.03
    L1/TEX Cache Throughput                                                              %                          44.66
    L2 Cache Throughput                                                                  %                          35.05
    SM Active Cycles                                                                 cycle                       21230.37
    Compute (SM) [%]                                                                     %                          56.10
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        2048
    Registers Per Thread                                                   register/thread                             33
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                             24
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         262144
    Waves Per SM                                                                                                     1.58
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 751 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             12
    Block Limit Shared Mem                                                           block                             28
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          58.21
    Achieved Active Warps Per SM                                                      warp                          37.26
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers The difference     
          between calculated theoretical (75.0%) and measured achieved occupancy (58.2%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void at::native::<unnamed>::vectorized_layer_norm_kernel<c10::Half, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *), 2022-Dec-04 20:05:29, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.50
    SM Frequency                                                             cycle/nsecond                           1.04
    Elapsed Cycles                                                                   cycle                          25387
    Memory [%]                                                                           %                          37.64
    DRAM Throughput                                                                      %                          18.03
    Duration                                                                       usecond                          24.26
    L1/TEX Cache Throughput                                                              %                          44.76
    L2 Cache Throughput                                                                  %                          35.13
    SM Active Cycles                                                                 cycle                       21207.71
    Compute (SM) [%]                                                                     %                          55.16
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        2048
    Registers Per Thread                                                   register/thread                             33
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                             24
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         262144
    Waves Per SM                                                                                                     1.58
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 751 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             12
    Block Limit Shared Mem                                                           block                             28
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          57.95
    Achieved Active Warps Per SM                                                      warp                          37.09
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers The difference     
          between calculated theoretical (75.0%) and measured achieved occupancy (57.9%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void at::native::<unnamed>::vectorized_layer_norm_kernel<c10::Half, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *), 2022-Dec-04 20:05:29, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.53
    SM Frequency                                                             cycle/nsecond                           1.06
    Elapsed Cycles                                                                   cycle                          24959
    Memory [%]                                                                           %                          38.28
    DRAM Throughput                                                                      %                          18.37
    Duration                                                                       usecond                          23.33
    L1/TEX Cache Throughput                                                              %                          44.78
    L2 Cache Throughput                                                                  %                          35.73
    SM Active Cycles                                                                 cycle                       21169.91
    Compute (SM) [%]                                                                     %                          56.18
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        2048
    Registers Per Thread                                                   register/thread                             33
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                             24
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         262144
    Waves Per SM                                                                                                     1.58
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 751 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             12
    Block Limit Shared Mem                                                           block                             28
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          57.92
    Achieved Active Warps Per SM                                                      warp                          37.07
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers The difference     
          between calculated theoretical (75.0%) and measured achieved occupancy (57.9%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void at::native::<unnamed>::vectorized_layer_norm_kernel<c10::Half, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *), 2022-Dec-04 20:05:30, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.50
    SM Frequency                                                             cycle/nsecond                           1.04
    Elapsed Cycles                                                                   cycle                          25018
    Memory [%]                                                                           %                          38.25
    DRAM Throughput                                                                      %                          18.33
    Duration                                                                       usecond                          23.90
    L1/TEX Cache Throughput                                                              %                          44.59
    L2 Cache Throughput                                                                  %                          35.92
    SM Active Cycles                                                                 cycle                       21271.29
    Compute (SM) [%]                                                                     %                          56.09
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        2048
    Registers Per Thread                                                   register/thread                             33
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                             24
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         262144
    Waves Per SM                                                                                                     1.58
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 751 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             12
    Block Limit Shared Mem                                                           block                             28
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          57.80
    Achieved Active Warps Per SM                                                      warp                          36.99
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers The difference     
          between calculated theoretical (75.0%) and measured achieved occupancy (57.8%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void at::native::<unnamed>::vectorized_layer_norm_kernel<c10::Half, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *), 2022-Dec-04 20:05:31, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.48
    SM Frequency                                                             cycle/nsecond                           1.03
    Elapsed Cycles                                                                   cycle                          24832
    Memory [%]                                                                           %                          38.46
    DRAM Throughput                                                                      %                          18.46
    Duration                                                                       usecond                          23.97
    L1/TEX Cache Throughput                                                              %                          44.68
    L2 Cache Throughput                                                                  %                          36.00
    SM Active Cycles                                                                 cycle                       21209.58
    Compute (SM) [%]                                                                     %                          56.47
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        2048
    Registers Per Thread                                                   register/thread                             33
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                             24
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         262144
    Waves Per SM                                                                                                     1.58
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 751 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             12
    Block Limit Shared Mem                                                           block                             28
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          58.06
    Achieved Active Warps Per SM                                                      warp                          37.16
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers The difference     
          between calculated theoretical (75.0%) and measured achieved occupancy (58.1%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void at::native::<unnamed>::vectorized_layer_norm_kernel<c10::Half, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *), 2022-Dec-04 20:05:32, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.46
    SM Frequency                                                             cycle/nsecond                           1.01
    Elapsed Cycles                                                                   cycle                          25055
    Memory [%]                                                                           %                          38.18
    DRAM Throughput                                                                      %                          18.29
    Duration                                                                       usecond                          24.51
    L1/TEX Cache Throughput                                                              %                          44.45
    L2 Cache Throughput                                                                  %                          35.32
    SM Active Cycles                                                                 cycle                       21341.44
    Compute (SM) [%]                                                                     %                          56.03
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        2048
    Registers Per Thread                                                   register/thread                             33
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                             24
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         262144
    Waves Per SM                                                                                                     1.58
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 751 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             12
    Block Limit Shared Mem                                                           block                             28
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          58.19
    Achieved Active Warps Per SM                                                      warp                          37.24
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers The difference     
          between calculated theoretical (75.0%) and measured achieved occupancy (58.2%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void at::native::<unnamed>::vectorized_layer_norm_kernel<c10::Half, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *), 2022-Dec-04 20:05:33, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.51
    SM Frequency                                                             cycle/nsecond                           1.05
    Elapsed Cycles                                                                   cycle                          25078
    Memory [%]                                                                           %                          38.11
    DRAM Throughput                                                                      %                          18.27
    Duration                                                                       usecond                          23.71
    L1/TEX Cache Throughput                                                              %                          44.62
    L2 Cache Throughput                                                                  %                          34.99
    SM Active Cycles                                                                 cycle                       21243.07
    Compute (SM) [%]                                                                     %                          55.93
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        2048
    Registers Per Thread                                                   register/thread                             33
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                             24
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         262144
    Waves Per SM                                                                                                     1.58
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 751 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             12
    Block Limit Shared Mem                                                           block                             28
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          58.11
    Achieved Active Warps Per SM                                                      warp                          37.19
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers The difference     
          between calculated theoretical (75.0%) and measured achieved occupancy (58.1%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void at::native::<unnamed>::vectorized_layer_norm_kernel<c10::Half, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *), 2022-Dec-04 20:05:34, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.48
    SM Frequency                                                             cycle/nsecond                           1.02
    Elapsed Cycles                                                                   cycle                          24924
    Memory [%]                                                                           %                          38.38
    DRAM Throughput                                                                      %                          18.40
    Duration                                                                       usecond                          24.13
    L1/TEX Cache Throughput                                                              %                          44.62
    L2 Cache Throughput                                                                  %                          35.98
    SM Active Cycles                                                                 cycle                       21257.08
    Compute (SM) [%]                                                                     %                          56.28
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        2048
    Registers Per Thread                                                   register/thread                             33
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                             24
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         262144
    Waves Per SM                                                                                                     1.58
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 751 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             12
    Block Limit Shared Mem                                                           block                             28
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          57.90
    Achieved Active Warps Per SM                                                      warp                          37.06
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers The difference     
          between calculated theoretical (75.0%) and measured achieved occupancy (57.9%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void at::native::<unnamed>::vectorized_layer_norm_kernel<c10::Half, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *), 2022-Dec-04 20:05:35, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.51
    SM Frequency                                                             cycle/nsecond                           1.04
    Elapsed Cycles                                                                   cycle                          24976
    Memory [%]                                                                           %                          38.27
    DRAM Throughput                                                                      %                          18.35
    Duration                                                                       usecond                          23.74
    L1/TEX Cache Throughput                                                              %                          44.76
    L2 Cache Throughput                                                                  %                          35.82
    SM Active Cycles                                                                 cycle                       21169.40
    Compute (SM) [%]                                                                     %                          56.18
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        2048
    Registers Per Thread                                                   register/thread                             33
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                             24
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         262144
    Waves Per SM                                                                                                     1.58
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 751 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             12
    Block Limit Shared Mem                                                           block                             28
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          57.92
    Achieved Active Warps Per SM                                                      warp                          37.07
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers The difference     
          between calculated theoretical (75.0%) and measured achieved occupancy (57.9%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void at::native::<unnamed>::vectorized_layer_norm_kernel<c10::Half, float>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *), 2022-Dec-04 20:05:36, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.51
    SM Frequency                                                             cycle/nsecond                           1.05
    Elapsed Cycles                                                                   cycle                          24928
    Memory [%]                                                                           %                          38.32
    DRAM Throughput                                                                      %                          18.38
    Duration                                                                       usecond                          23.62
    L1/TEX Cache Throughput                                                              %                          44.59
    L2 Cache Throughput                                                                  %                          34.70
    SM Active Cycles                                                                 cycle                       21259.45
    Compute (SM) [%]                                                                     %                          56.23
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        128
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        2048
    Registers Per Thread                                                   register/thread                             33
    Shared Memory Configuration Size                                                 Kbyte                          32.77
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                             byte/block                             24
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         262144
    Waves Per SM                                                                                                     1.58
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 751 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             12
    Block Limit Shared Mem                                                           block                             28
    Block Limit Warps                                                                block                             16
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          58.16
    Achieved Active Warps Per SM                                                      warp                          37.22
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers The difference     
          between calculated theoretical (75.0%) and measured achieved occupancy (58.2%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  CudaCodeGen::kernel1(CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<float, (int)2>, CudaCodeGen::Tensor<float, (int)2>), 2022-Dec-04 20:05:37, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.52
    SM Frequency                                                             cycle/nsecond                           1.06
    Elapsed Cycles                                                                   cycle                          29821
    Memory [%]                                                                           %                          23.59
    DRAM Throughput                                                                      %                          15.46
    Duration                                                                       usecond                          27.94
    L1/TEX Cache Throughput                                                              %                          28.25
    L2 Cache Throughput                                                                  %                          29.60
    SM Active Cycles                                                                 cycle                       24791.50
    Compute (SM) [%]                                                                     %                          40.77
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        1024
    Registers Per Thread                                                   register/thread                             40
    Shared Memory Configuration Size                                                 Kbyte                          65.54
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                           6.14
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         524288
    Waves Per SM                                                                                                     3.16
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              3
    Block Limit Shared Mem                                                           block                              9
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          66.26
    Achieved Active Warps Per SM                                                      warp                          42.40
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers See the CUDA Best  
          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      
          details on optimizing occupancy.                                                                              

  CudaCodeGen::kernel1(CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<float, (int)2>, CudaCodeGen::Tensor<float, (int)2>), 2022-Dec-04 20:05:38, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.50
    SM Frequency                                                             cycle/nsecond                           1.05
    Elapsed Cycles                                                                   cycle                          29748
    Memory [%]                                                                           %                          23.66
    DRAM Throughput                                                                      %                          15.48
    Duration                                                                       usecond                          28.16
    L1/TEX Cache Throughput                                                              %                          28.21
    L2 Cache Throughput                                                                  %                          29.79
    SM Active Cycles                                                                 cycle                       24821.48
    Compute (SM) [%]                                                                     %                          40.89
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        1024
    Registers Per Thread                                                   register/thread                             40
    Shared Memory Configuration Size                                                 Kbyte                          65.54
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                           6.14
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         524288
    Waves Per SM                                                                                                     3.16
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              3
    Block Limit Shared Mem                                                           block                              9
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          66.24
    Achieved Active Warps Per SM                                                      warp                          42.40
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers See the CUDA Best  
          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      
          details on optimizing occupancy.                                                                              

  CudaCodeGen::kernel1(CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<float, (int)2>, CudaCodeGen::Tensor<float, (int)2>), 2022-Dec-04 20:05:39, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.50
    SM Frequency                                                             cycle/nsecond                           1.05
    Elapsed Cycles                                                                   cycle                          29548
    Memory [%]                                                                           %                          23.77
    DRAM Throughput                                                                      %                          15.59
    Duration                                                                       usecond                          28.13
    L1/TEX Cache Throughput                                                              %                          28.27
    L2 Cache Throughput                                                                  %                          29.28
    SM Active Cycles                                                                 cycle                       24723.12
    Compute (SM) [%]                                                                     %                          41.17
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        1024
    Registers Per Thread                                                   register/thread                             40
    Shared Memory Configuration Size                                                 Kbyte                          65.54
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                           6.14
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         524288
    Waves Per SM                                                                                                     3.16
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              3
    Block Limit Shared Mem                                                           block                              9
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          66.23
    Achieved Active Warps Per SM                                                      warp                          42.39
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers See the CUDA Best  
          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      
          details on optimizing occupancy.                                                                              

  CudaCodeGen::kernel1(CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<float, (int)2>, CudaCodeGen::Tensor<float, (int)2>), 2022-Dec-04 20:05:40, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.48
    SM Frequency                                                             cycle/nsecond                           1.03
    Elapsed Cycles                                                                   cycle                          29428
    Memory [%]                                                                           %                          23.86
    DRAM Throughput                                                                      %                          15.66
    Duration                                                                       usecond                          28.32
    L1/TEX Cache Throughput                                                              %                          28.24
    L2 Cache Throughput                                                                  %                          29.26
    SM Active Cycles                                                                 cycle                       24755.30
    Compute (SM) [%]                                                                     %                          41.30
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        1024
    Registers Per Thread                                                   register/thread                             40
    Shared Memory Configuration Size                                                 Kbyte                          65.54
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                           6.14
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         524288
    Waves Per SM                                                                                                     3.16
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              3
    Block Limit Shared Mem                                                           block                              9
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          66.24
    Achieved Active Warps Per SM                                                      warp                          42.39
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers See the CUDA Best  
          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      
          details on optimizing occupancy.                                                                              

  CudaCodeGen::kernel1(CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<float, (int)2>, CudaCodeGen::Tensor<float, (int)2>), 2022-Dec-04 20:05:41, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.49
    SM Frequency                                                             cycle/nsecond                           1.04
    Elapsed Cycles                                                                   cycle                          29675
    Memory [%]                                                                           %                          23.67
    DRAM Throughput                                                                      %                          15.52
    Duration                                                                       usecond                          28.42
    L1/TEX Cache Throughput                                                              %                          28.30
    L2 Cache Throughput                                                                  %                          28.84
    SM Active Cycles                                                                 cycle                       24719.45
    Compute (SM) [%]                                                                     %                          40.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        1024
    Registers Per Thread                                                   register/thread                             40
    Shared Memory Configuration Size                                                 Kbyte                          65.54
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                           6.14
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         524288
    Waves Per SM                                                                                                     3.16
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              3
    Block Limit Shared Mem                                                           block                              9
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          66.29
    Achieved Active Warps Per SM                                                      warp                          42.42
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers See the CUDA Best  
          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      
          details on optimizing occupancy.                                                                              

  CudaCodeGen::kernel1(CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<float, (int)2>, CudaCodeGen::Tensor<float, (int)2>), 2022-Dec-04 20:05:41, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.50
    SM Frequency                                                             cycle/nsecond                           1.05
    Elapsed Cycles                                                                   cycle                          29440
    Memory [%]                                                                           %                          23.82
    DRAM Throughput                                                                      %                          15.64
    Duration                                                                       usecond                          27.94
    L1/TEX Cache Throughput                                                              %                          28.14
    L2 Cache Throughput                                                                  %                          29.51
    SM Active Cycles                                                                 cycle                       24810.79
    Compute (SM) [%]                                                                     %                          41.29
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        1024
    Registers Per Thread                                                   register/thread                             40
    Shared Memory Configuration Size                                                 Kbyte                          65.54
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                           6.14
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         524288
    Waves Per SM                                                                                                     3.16
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              3
    Block Limit Shared Mem                                                           block                              9
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          66.18
    Achieved Active Warps Per SM                                                      warp                          42.35
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers See the CUDA Best  
          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      
          details on optimizing occupancy.                                                                              

  CudaCodeGen::kernel1(CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<float, (int)2>, CudaCodeGen::Tensor<float, (int)2>), 2022-Dec-04 20:05:42, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.50
    SM Frequency                                                             cycle/nsecond                           1.05
    Elapsed Cycles                                                                   cycle                          29589
    Memory [%]                                                                           %                          23.71
    DRAM Throughput                                                                      %                          15.58
    Duration                                                                       usecond                          28.06
    L1/TEX Cache Throughput                                                              %                          28.23
    L2 Cache Throughput                                                                  %                          29.45
    SM Active Cycles                                                                 cycle                       24737.93
    Compute (SM) [%]                                                                     %                          41.10
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        1024
    Registers Per Thread                                                   register/thread                             40
    Shared Memory Configuration Size                                                 Kbyte                          65.54
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                           6.14
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         524288
    Waves Per SM                                                                                                     3.16
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              3
    Block Limit Shared Mem                                                           block                              9
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          66.24
    Achieved Active Warps Per SM                                                      warp                          42.39
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers See the CUDA Best  
          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      
          details on optimizing occupancy.                                                                              

  CudaCodeGen::kernel1(CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<float, (int)2>, CudaCodeGen::Tensor<float, (int)2>), 2022-Dec-04 20:05:43, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.50
    SM Frequency                                                             cycle/nsecond                           1.05
    Elapsed Cycles                                                                   cycle                          29572
    Memory [%]                                                                           %                          23.71
    DRAM Throughput                                                                      %                          15.57
    Duration                                                                       usecond                          28.10
    L1/TEX Cache Throughput                                                              %                          28.18
    L2 Cache Throughput                                                                  %                          28.55
    SM Active Cycles                                                                 cycle                       24789.50
    Compute (SM) [%]                                                                     %                          41.08
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        1024
    Registers Per Thread                                                   register/thread                             40
    Shared Memory Configuration Size                                                 Kbyte                          65.54
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                           6.14
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         524288
    Waves Per SM                                                                                                     3.16
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              3
    Block Limit Shared Mem                                                           block                              9
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          66.23
    Achieved Active Warps Per SM                                                      warp                          42.39
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers See the CUDA Best  
          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      
          details on optimizing occupancy.                                                                              

  CudaCodeGen::kernel1(CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<float, (int)2>, CudaCodeGen::Tensor<float, (int)2>), 2022-Dec-04 20:05:44, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.50
    SM Frequency                                                             cycle/nsecond                           1.05
    Elapsed Cycles                                                                   cycle                          29510
    Memory [%]                                                                           %                          23.76
    DRAM Throughput                                                                      %                          15.62
    Duration                                                                       usecond                          27.94
    L1/TEX Cache Throughput                                                              %                          28.25
    L2 Cache Throughput                                                                  %                          28.91
    SM Active Cycles                                                                 cycle                       24713.38
    Compute (SM) [%]                                                                     %                          41.19
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        1024
    Registers Per Thread                                                   register/thread                             40
    Shared Memory Configuration Size                                                 Kbyte                          65.54
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                           6.14
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         524288
    Waves Per SM                                                                                                     3.16
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              3
    Block Limit Shared Mem                                                           block                              9
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          66.17
    Achieved Active Warps Per SM                                                      warp                          42.35
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers See the CUDA Best  
          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      
          details on optimizing occupancy.                                                                              

  CudaCodeGen::kernel1(CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<float, (int)2>, CudaCodeGen::Tensor<float, (int)2>), 2022-Dec-04 20:05:45, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.50
    SM Frequency                                                             cycle/nsecond                           1.05
    Elapsed Cycles                                                                   cycle                          29561
    Memory [%]                                                                           %                          23.72
    DRAM Throughput                                                                      %                          15.59
    Duration                                                                       usecond                          28.03
    L1/TEX Cache Throughput                                                              %                          28.21
    L2 Cache Throughput                                                                  %                          29.45
    SM Active Cycles                                                                 cycle                       24737.97
    Compute (SM) [%]                                                                     %                          41.15
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        1024
    Registers Per Thread                                                   register/thread                             40
    Shared Memory Configuration Size                                                 Kbyte                          65.54
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                           6.14
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         524288
    Waves Per SM                                                                                                     3.16
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              3
    Block Limit Shared Mem                                                           block                              9
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          66.23
    Achieved Active Warps Per SM                                                      warp                          42.39
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers See the CUDA Best  
          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      
          details on optimizing occupancy.                                                                              

  CudaCodeGen::kernel1(CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<float, (int)2>, CudaCodeGen::Tensor<float, (int)2>), 2022-Dec-04 20:05:46, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.49
    SM Frequency                                                             cycle/nsecond                           1.04
    Elapsed Cycles                                                                   cycle                          29337
    Memory [%]                                                                           %                          23.95
    DRAM Throughput                                                                      %                          15.68
    Duration                                                                       usecond                          28.16
    L1/TEX Cache Throughput                                                              %                          28.30
    L2 Cache Throughput                                                                  %                          29.03
    SM Active Cycles                                                                 cycle                       24723.31
    Compute (SM) [%]                                                                     %                          41.44
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        1024
    Registers Per Thread                                                   register/thread                             40
    Shared Memory Configuration Size                                                 Kbyte                          65.54
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                           6.14
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         524288
    Waves Per SM                                                                                                     3.16
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              3
    Block Limit Shared Mem                                                           block                              9
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          66.24
    Achieved Active Warps Per SM                                                      warp                          42.39
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers See the CUDA Best  
          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      
          details on optimizing occupancy.                                                                              

  CudaCodeGen::kernel1(CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<float, (int)2>, CudaCodeGen::Tensor<float, (int)2>), 2022-Dec-04 20:05:47, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.48
    SM Frequency                                                             cycle/nsecond                           1.03
    Elapsed Cycles                                                                   cycle                          29526
    Memory [%]                                                                           %                          23.77
    DRAM Throughput                                                                      %                          15.60
    Duration                                                                       usecond                          28.48
    L1/TEX Cache Throughput                                                              %                          28.15
    L2 Cache Throughput                                                                  %                          28.53
    SM Active Cycles                                                                 cycle                       24827.26
    Compute (SM) [%]                                                                     %                          41.16
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        1024
    Registers Per Thread                                                   register/thread                             40
    Shared Memory Configuration Size                                                 Kbyte                          65.54
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                           6.14
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         524288
    Waves Per SM                                                                                                     3.16
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              3
    Block Limit Shared Mem                                                           block                              9
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          66.19
    Achieved Active Warps Per SM                                                      warp                          42.36
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers See the CUDA Best  
          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      
          details on optimizing occupancy.                                                                              

  CudaCodeGen::kernel1(CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)1>, CudaCodeGen::Tensor<CudaCodeGen::__half, (int)2>, CudaCodeGen::Tensor<float, (int)2>, CudaCodeGen::Tensor<float, (int)2>), 2022-Dec-04 20:05:48, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/nsecond                           1.51
    SM Frequency                                                             cycle/nsecond                           1.06
    Elapsed Cycles                                                                   cycle                          29650
    Memory [%]                                                                           %                          23.67
    DRAM Throughput                                                                      %                          15.54
    Duration                                                                       usecond                          27.94
    L1/TEX Cache Throughput                                                              %                          28.33
    L2 Cache Throughput                                                                  %                          29.55
    SM Active Cycles                                                                 cycle                       24670.49
    Compute (SM) [%]                                                                     %                          41.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                        512
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        1024
    Registers Per Thread                                                   register/thread                             40
    Shared Memory Configuration Size                                                 Kbyte                          65.54
    Driver Shared Memory Per Block                                             Kbyte/block                           1.02
    Dynamic Shared Memory Per Block                                            Kbyte/block                           6.14
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                         524288
    Waves Per SM                                                                                                     3.16
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              3
    Block Limit Shared Mem                                                           block                              9
    Block Limit Warps                                                                block                              4
    Theoretical Active Warps per SM                                                   warp                             48
    Theoretical Occupancy                                                                %                             75
    Achieved Occupancy                                                                   %                          66.25
    Achieved Active Warps Per SM                                                      warp                          42.40
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (75.0%) is limited by the number of required registers See the CUDA Best  
          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      
          details on optimizing occupancy.                                                                              

